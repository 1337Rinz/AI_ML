{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1337Rinz/AI_ML/blob/main/lab07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8DTumV58Nki",
        "outputId": "16eafe66-7f05-4531-a542-dfadd461f59c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf-nightly\n",
            "  Downloading tf_nightly-2.15.0.dev20230809-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (491.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.9/491.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.8.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.56.2)\n",
            "Collecting tb-nightly~=2.14.0.a (from tf-nightly)\n",
            "  Downloading tb_nightly-2.14.0a20230808-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tf-estimator-nightly~=2.14.0.dev (from tf-nightly)\n",
            "  Downloading tf_estimator_nightly-2.14.0.dev2023080308-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.9/440.9 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-nightly~=2.14.0.dev (from tf-nightly)\n",
            "  Downloading keras_nightly-2.14.0.dev2023080207-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tf-nightly) (0.41.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.14.0.a->tf-nightly) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.14.0.a->tf-nightly) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.14.0.a->tf-nightly) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tb-nightly~=2.14.0.a->tf-nightly) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.14.0.a->tf-nightly) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.14.0.a->tf-nightly) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.14.0.a->tf-nightly) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.14.0.a->tf-nightly) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tb-nightly~=2.14.0.a->tf-nightly) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly~=2.14.0.a->tf-nightly) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tb-nightly~=2.14.0.a->tf-nightly) (3.2.2)\n",
            "Installing collected packages: tf-estimator-nightly, keras-nightly, tb-nightly, tf-nightly\n",
            "Successfully installed keras-nightly-2.14.0.dev2023080207 tb-nightly-2.14.0a20230808 tf-estimator-nightly-2.14.0.dev2023080308 tf-nightly-2.15.0.dev20230809\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.14)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tf-nightly\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "1umDIV2c8-b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4239f96b-01f4-453f-ab2e-22abe8f66ad1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "  MAX_LEN = 256\n",
        "  BATCH_SIZE = 32\n",
        "  LR = 0.001\n",
        "  VOCAB_SIZE = 30000\n",
        "  EMBED_DIM = 120\n",
        "  NUM_HEAD = 8\n",
        "  FF_DIM = 128\n",
        "  NUM_LAYERS = 1\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "7tjCJxAG9hUC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PsKDyWL9_xC",
        "outputId": "2e89b0e7-c80d-4c17-b322-1b985f3bc280"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  58.1M      0  0:00:01  0:00:01 --:--:-- 58.1M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf /content/aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "SFIFrbWC-SKq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_list_from_files(files):\n",
        "  text_list = []\n",
        "  for name in files:\n",
        "    with open(name) as f:\n",
        "      for line in f:\n",
        "        text_list.append(line)\n",
        "  return text_list\n",
        "\n",
        "\n",
        "def get_data_from_text_files(folder_name):\n",
        "  pos_files = glob.glob(\"aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
        "  pos_texts = get_text_list_from_files(pos_files)\n",
        "\n",
        "  neg_files = glob.glob(\"aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
        "  neg_texts = get_text_list_from_files(neg_files)\n",
        "\n",
        "  df = pd.DataFrame({\n",
        "      \"review\": pos_texts + neg_texts,\n",
        "      \"sentiment\": [0] * len(pos_texts) + [1] * len(neg_texts)\n",
        "  })\n",
        "\n",
        "  df = df.sample(len(df)).reset_index(drop=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "itKe4wVj-mOj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = get_data_from_text_files(\"train\")\n",
        "test_df = get_data_from_text_files(\"test\")\n",
        "\n",
        "all_data = train_df.append(test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7QkuZoG_uL7",
        "outputId": "3e2219d1-677b-4a53-8e58-5a2a4bb15c5a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-8d9aac150f9d>:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_data = train_df.append(test_df)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "IM0qSLMmFd9s",
        "outputId": "b57e318b-60a5-43bb-8913-c5cd57979607"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  review  sentiment\n",
              "0      I decided to watch this one because it's been ...          1\n",
              "1      Mesmerizing, breathtaking and horrifying, this...          0\n",
              "2      I really hope that Concorde/New Horizons wasn'...          1\n",
              "3      This almost unknown gem was based on a French ...          0\n",
              "4      Aside from the horrendous acting and the ridic...          1\n",
              "...                                                  ...        ...\n",
              "24995  I think this movie is underrated. To me it fel...          0\n",
              "24996  Tyra Banks needs to teach these girls that it'...          1\n",
              "24997  This movie was difficult for me to watch. Stan...          1\n",
              "24998  I kid you not. Yes, \"Who's That Girl\" has the ...          0\n",
              "24999  I don't usually like TV movies, I reckon that ...          1\n",
              "\n",
              "[50000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-442f5e40-3a2c-4892-80ec-a4ba0ca1ef06\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I decided to watch this one because it's been ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mesmerizing, breathtaking and horrifying, this...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I really hope that Concorde/New Horizons wasn'...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This almost unknown gem was based on a French ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Aside from the horrendous acting and the ridic...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>I think this movie is underrated. To me it fel...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>Tyra Banks needs to teach these girls that it'...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>This movie was difficult for me to watch. Stan...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>I kid you not. Yes, \"Who's That Girl\" has the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>I don't usually like TV movies, I reckon that ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-442f5e40-3a2c-4892-80ec-a4ba0ca1ef06')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-2851ab71-cc02-48f8-a067-8f00b2237ae4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2851ab71-cc02-48f8-a067-8f00b2237ae4')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-2851ab71-cc02-48f8-a067-8f00b2237ae4 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-442f5e40-3a2c-4892-80ec-a4ba0ca1ef06 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-442f5e40-3a2c-4892-80ec-a4ba0ca1ef06');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standarization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "  return tf.strings.regex_replace(stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\")\n",
        "\n",
        "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
        "  vectorize_layer = TextVectorization(\n",
        "      max_tokens=vocab_size,\n",
        "      output_mode=\"int\",\n",
        "      standardize=custom_standarization,\n",
        "      output_sequence_length=max_seq,\n",
        "  )\n",
        "  vectorize_layer.adapt(texts)\n",
        "\n",
        "  vocab = vectorize_layer.get_vocabulary()\n",
        "  vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
        "  vectorize_layer.set_vocabulary(vocab)\n",
        "  return vectorize_layer\n",
        "\n",
        "\n",
        "vectorize_layer = get_vectorize_layer(\n",
        "    all_data.review.values.tolist(),\n",
        "    config.VOCAB_SIZE,\n",
        "    config.MAX_LEN,\n",
        "    special_tokens=[\"[mask]\"]\n",
        ")\n",
        "\n",
        "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n"
      ],
      "metadata": {
        "id": "P1jQZmUrACRq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(texts):\n",
        "  encoded_texts = vectorize_layer(texts)\n",
        "  return encoded_texts.numpy()\n",
        "\n",
        "def get_masked_input_and_labels(encoded_texts):\n",
        "  inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
        "  inp_mask[encoded_texts <= 2 ] = False\n",
        "\n",
        "  labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
        "  labels[inp_mask] = encoded_texts[inp_mask]\n",
        "\n",
        "  encoded_texts_masked = np.copy(encoded_texts)\n",
        "\n",
        "  inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
        "  encoded_texts_masked[inp_mask_2mask] = mask_token_id\n",
        "\n",
        "  inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape)< 1/9)\n",
        "  encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
        "      3, mask_token_id, inp_mask_2random.sum()\n",
        "  )\n",
        "  sample_weights = np.ones(labels.shape)\n",
        "  sample_weights[labels == -1 ] = 0\n",
        "\n",
        "  y_labels = np.copy(encoded_texts)\n",
        "  return encoded_texts_masked, y_labels, sample_weights"
      ],
      "metadata": {
        "id": "M_EpH2QHB8bk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = encode(train_df.review.values)\n",
        "y_train = train_df.sentiment.values\n",
        "train_classifier_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    .shuffle(1000)\n",
        "    .batch(config.BATCH_SIZE)\n",
        ")"
      ],
      "metadata": {
        "id": "8ZqbMprcAxpU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = encode(test_df.review.values)\n",
        "y_test = test_df.sentiment.values\n",
        "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(config.BATCH_SIZE)\n",
        "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices((test_df.review.values, y_test)).batch(config.BATCH_SIZE)"
      ],
      "metadata": {
        "id": "IhxzCuZPG7jt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_all_review = encode(all_data.review.values)\n",
        "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
        "    x_all_review\n",
        ")\n",
        "\n",
        "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_masked_train, y_masked_labels, sample_weights)\n",
        ")\n",
        "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)"
      ],
      "metadata": {
        "id": "QidYIVkaHvAN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_module(query, key, value, i):\n",
        "  attention_output = layers.MultiHeadAttention(\n",
        "      num_heads=config.NUM_HEAD,\n",
        "      key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
        "      name=\"encoder_{}/multiheadattention\".format(i),\n",
        "  )(query, key, value)\n",
        "  attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(attention_output)\n",
        "  attention_output = layers.LayerNormalization(\n",
        "      epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n",
        "  )(query + attention_output)\n",
        "\n",
        "  ffn = keras.Sequential(\n",
        "      [\n",
        "          layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
        "          layers.Dense(config.EMBED_DIM),\n",
        "      ],\n",
        "      name=\"encoder_{}/ffn\".format(i),\n",
        "  )\n",
        "\n",
        "  ffn_output = ffn(attention_output)\n",
        "  ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(ffn_output)\n",
        "  sequence_output = layers.LayerNormalization(\n",
        "      epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
        "  )(attention_output+ffn_output)\n",
        "  return sequence_output"
      ],
      "metadata": {
        "id": "JzrBgzFrIRBm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos_encoding_matrix(max_len, d_emb):\n",
        "  pos_enc = np.array(\n",
        "      [\n",
        "          [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
        "        if pos != 0\n",
        "        else np.zeros(d_emb)\n",
        "        for pos in range(max_len)\n",
        "      ]\n",
        "  )\n",
        "  pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])\n",
        "  pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])\n",
        "  return pos_enc\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
        "    reduction=tf.keras.losses.Reduction.NONE\n",
        ")\n",
        "\n",
        "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "class MaskedLanguageModel(tf.keras.Model):\n",
        "  def train_step(self, inputs):\n",
        "    if len(inputs) == 3:\n",
        "      features, labels, sample_weight = inputs\n",
        "    else:\n",
        "      features, labels = inputs\n",
        "      sample_weight = None\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(features, training=True)\n",
        "      loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
        "\n",
        "      trainable_vars = self.trainable_variables\n",
        "      gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "      self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "      loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
        "\n",
        "      return {\"loss\": loss_tracker.result()}\n",
        "\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return [loss_tracker]\n",
        "\n",
        "def create_masked_language_bert_model():\n",
        "  inputs = layers.Input((config.MAX_LEN), dtype=tf.int64)\n",
        "\n",
        "  word_embeddings = layers.Embedding(\n",
        "      config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
        "  )(inputs)\n",
        "  position_embeddings = layers.Embedding(\n",
        "      input_dim=config.MAX_LEN,\n",
        "      output_dim=config.EMBED_DIM,\n",
        "      weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
        "      name=\"position_embedding\"\n",
        "  )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
        "  embeddings = word_embeddings + position_embeddings\n",
        "\n",
        "  encoder_output = embeddings\n",
        "  for i in range(config.NUM_LAYERS):\n",
        "    encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
        "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(encoder_output)\n",
        "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
        "    mlm_model.compile(optimizer=optimizer)\n",
        "    return mlm_model"
      ],
      "metadata": {
        "id": "9_j6kWo3MUFW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
        "token2id = {y:x for x, y in id2token.items()}"
      ],
      "metadata": {
        "id": "NEoN3ZkHRqxP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedTextGenerator(keras.callbacks.Callback):\n",
        "  def __init__(self, sample_tokens, top_k=5):\n",
        "    self.sample_tokens = sample_tokens\n",
        "    self.k = top_k\n",
        "\n",
        "  def decode(self, tokens):\n",
        "    return \" \".join([id2token[t] for t in tokens if t !=0])\n",
        "\n",
        "  def convert_ids_to_tokens(self, id):\n",
        "    return id2token[id]\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    prediction = self.model.predict(self.sample_tokens)\n",
        "\n",
        "    masked_index = np.where(self.sample_tokens == mask_token_id)\n",
        "    masked_index = masked_index[1]\n",
        "    mask_prediction = prediction[0][masked_index]\n",
        "\n",
        "    top_indices = masked_index[0].argsort()[-self.k :][::-1]\n",
        "    values = mask_prediction[0][top_indices]\n",
        "\n",
        "    for i in range(len(top_indices)):\n",
        "      p = top_indices[i]\n",
        "      v = values[i]\n",
        "\n",
        "      tokens = np.copy(sample_tokens[0])\n",
        "      tokens[masked_index[0]] = p\n",
        "      result = {\n",
        "          \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
        "          \"prediction\": self.decode(tokens),\n",
        "          \"probability\": v,\n",
        "          \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
        "      }\n",
        "      pprint(result)\n"
      ],
      "metadata": {
        "id": "QsJGzmKsR1V3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
        "generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
        "\n",
        "bert_masked_model = create_masked_language_bert_model()\n",
        "bert_masked_model.summary()\n"
      ],
      "metadata": {
        "id": "gT3o0w97VEwB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66ab892c-b75e-481b-d486-4c04fb82bbe6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"masked_bert_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 256)]                0         []                            \n",
            "                                                                                                  \n",
            " word_embedding (Embedding)  (None, 256, 120)             3600000   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOp  (None, 256, 120)             0         ['word_embedding[0][0]']      \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " encoder_0/multiheadattenti  (None, 256, 120)             58080     ['tf.__operators__.add[0][0]',\n",
            " on (MultiHeadAttention)                                             'tf.__operators__.add[0][0]',\n",
            "                                                                     'tf.__operators__.add[0][0]']\n",
            "                                                                                                  \n",
            " encoder_0/att_dropout (Dro  (None, 256, 120)             0         ['encoder_0/multiheadattention\n",
            " pout)                                                              [0][0]']                      \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TF  (None, 256, 120)             0         ['tf.__operators__.add[0][0]',\n",
            " OpLambda)                                                           'encoder_0/att_dropout[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " encoder_0/att_layernormali  (None, 256, 120)             240       ['tf.__operators__.add_1[0][0]\n",
            " zation (LayerNormalization                                         ']                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " encoder_0/ffn (Sequential)  (None, 256, 120)             30968     ['encoder_0/att_layernormaliza\n",
            "                                                                    tion[0][0]']                  \n",
            "                                                                                                  \n",
            " encoder_0/ffn_dropout (Dro  (None, 256, 120)             0         ['encoder_0/ffn[0][0]']       \n",
            " pout)                                                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TF  (None, 256, 120)             0         ['encoder_0/att_layernormaliza\n",
            " OpLambda)                                                          tion[0][0]',                  \n",
            "                                                                     'encoder_0/ffn_dropout[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " encoder_0/ffn_layernormali  (None, 256, 120)             240       ['tf.__operators__.add_2[0][0]\n",
            " zation (LayerNormalization                                         ']                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " mlm_cls (Dense)             (None, 256, 30000)           3630000   ['encoder_0/ffn_layernormaliza\n",
            "                                                                    tion[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7319528 (27.92 MB)\n",
            "Trainable params: 7319528 (27.92 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_masked_model.fit(mlm_ds, epochs=1, callbacks=[generator_callback])\n",
        "bert_masked_model.save(\"bert_mlm_imdb.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z6DRqaHjbz7",
        "outputId": "b5e7ddd9-6c97-4573-de3d-5b6d8430374d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 224ms/step\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': '',\n",
            " 'prediction': 'i have watched this and it was awesome',\n",
            " 'probability': 3.8476543e-08}\n",
            "1563/1563 [==============================] - 283s 174ms/step - loss: 6.9982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlm_model = keras.models.load_model(\"bert_mlm_imdb.h5\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel})\n",
        "pretrained_bert_model = tf.keras.Model(\n",
        "    mlm_model.input,\n",
        "    mlm_model.get_layer(\"encoder_0/ffn_layernormalization\").output\n",
        ")\n",
        "\n",
        "pretrained_bert_model.trainable = False\n",
        "\n",
        "def create_classifier_bert_model():\n",
        "  inputs = layers.Input((config.MAX_LEN), dtype=tf.int64)\n",
        "  sequence_output = pretrained_bert_model(inputs)\n",
        "  pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
        "  hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)\n",
        "\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
        "  classifier_model = keras.Model(inputs, outputs, name=\"classification\")\n",
        "  optimizer = keras.optimizers.Adam()\n",
        "  classifier_model.compile(\n",
        "      optimizer=optimizer, loss='binary_crossentropy', metrics=[\"accuracy\"]\n",
        "  )\n",
        "  return classifier_model\n"
      ],
      "metadata": {
        "id": "mYVQ8RPHjrTQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model = create_classifier_bert_model()\n",
        "classifier_model.summary()"
      ],
      "metadata": {
        "id": "dJRaL0hylodh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f5265fa-af65-4faa-e476-80f8791f6c61"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 256)]             0         \n",
            "                                                                 \n",
            " model (Functional)          (None, 256, 120)          3689528   \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 120)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                7744      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3697337 (14.10 MB)\n",
            "Trainable params: 7809 (30.50 KB)\n",
            "Non-trainable params: 3689528 (14.07 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.fit(\n",
        "    train_classifier_ds,\n",
        "    epochs=5,\n",
        "    validation_data=test_classifier_ds,\n",
        ")"
      ],
      "metadata": {
        "id": "JAISAum1ltUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e857b141-7975-4077-eba9-88263a8455c0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 16s 18ms/step - loss: 0.6982 - accuracy: 0.5667 - val_loss: 0.6571 - val_accuracy: 0.6121\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 17s 22ms/step - loss: 0.6632 - accuracy: 0.6065 - val_loss: 0.7241 - val_accuracy: 0.5459\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 13s 17ms/step - loss: 0.6613 - accuracy: 0.6089 - val_loss: 0.6420 - val_accuracy: 0.6318\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 14s 17ms/step - loss: 0.6521 - accuracy: 0.6180 - val_loss: 0.6809 - val_accuracy: 0.5798\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.6516 - accuracy: 0.6184 - val_loss: 0.6574 - val_accuracy: 0.6084\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a98e4b64340>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_end_to_end(model):\n",
        "  inputs_string = keras.Input(shape=(1,), dtype=\"string\")\n",
        "  indices = vectorize_layer(inputs_string)\n",
        "  outputs = model(indices)\n",
        "  end_to_end_model = keras.Model(inputs_string, outputs, name=\"end_to_end_model\")\n",
        "\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
        "  end_to_end_model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=\"binary_crossentropy\",\n",
        "      metrics=[\"accuracy\"]\n",
        "  )\n",
        "  return end_to_end_model"
      ],
      "metadata": {
        "id": "k2OOBZVZmI5l"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end_to_end_classification_model = get_end_to_end(classifier_model)\n",
        "end_to_end_classification_model.evaluate(test_raw_classifier_ds)\n"
      ],
      "metadata": {
        "id": "XNsvTJR0mpbq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1ffa9d9-da77-412c-cbe8-933f26bf7f08"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 10s 11ms/step - loss: 0.6574 - accuracy: 0.6084\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6573957800865173, 0.6083999872207642]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}